{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gan_cifar10.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harshalnishar/dcgan_cifar10/blob/master/gan_cifar10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYOC7XyQloQ7",
        "colab_type": "text"
      },
      "source": [
        "This notebook implements a deep convolutional generative adversarial network (DCGAN) to generate CIFAR-10 images from noise vector sampled from normal distribution.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yI1h1A9Nkat0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import urllib\n",
        "import zipfile \n",
        "import tarfile\n",
        "from six.moves import urllib\n",
        "import pickle\n",
        "from matplotlib import pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.contrib.training import HParams\n",
        "from IPython.display import Image\n",
        "\n",
        "hparams = HParams(\n",
        "    n_classes=10,  # number of different classes in dataset\n",
        "    batch_size=32,  # training batch size\n",
        "    dis_learning_rate=0.0001,\n",
        "    gen_learning_rate=0.0004,\n",
        "    n_epochs=50,  # number of epochs to train\n",
        "    data_dir='/tmp/cifar-data/',  # path to data directory\n",
        "    checkpoint_dir='/tmp/checkpoints/'  # path to model checkpoint directory\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5Gqw7aZ4KLO",
        "colab_type": "text"
      },
      "source": [
        "Functions for Reading CIFAR-10 Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2EpQ-iJN94G1",
        "colab_type": "code",
        "outputId": "a86da426-4f63-4d04-8e6a-bff9526ead08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "# URL for the data-set on the internet.\n",
        "data_url = \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
        "\n",
        "# Width and height of each image.\n",
        "img_size = 32\n",
        "\n",
        "# Number of channels in each image, 3 channels: Red, Green, Blue.\n",
        "num_channels = 3\n",
        "\n",
        "# Length of an image when flattened to a 1-dim array.\n",
        "img_size_flat = img_size * img_size * num_channels\n",
        "\n",
        "# Number of classes.\n",
        "num_classes = hparams.n_classes\n",
        "\n",
        "########################################################################\n",
        "# Various constants used to allocate arrays of the correct size.\n",
        "\n",
        "# Number of files for the training-set.\n",
        "_num_files_train = 5\n",
        "\n",
        "# Number of images for each batch-file in the training-set.\n",
        "_images_per_file = 10000\n",
        "\n",
        "# Total number of images in the training-set.\n",
        "# This is used to pre-allocate arrays for efficiency.\n",
        "_num_images_train = _num_files_train * _images_per_file\n",
        "\n",
        "########################################################################\n",
        "# Private functions for downloading, unpacking and loading data-files.\n",
        "\n",
        "\n",
        "def download(base_url, filename, download_dir):\n",
        "    \"\"\"\n",
        "    Download the given file if it does not already exist in the download_dir.\n",
        "    :param base_url: The internet URL without the filename.\n",
        "    :param filename: The filename that will be added to the base_url.\n",
        "    :param download_dir: Local directory for storing the file.\n",
        "    :return: Nothing.\n",
        "    \"\"\"\n",
        "\n",
        "    # Path for local file.\n",
        "    save_path = os.path.join(download_dir, filename)\n",
        "\n",
        "    # Check if the file already exists, otherwise we need to download it now.\n",
        "    if not os.path.exists(save_path):\n",
        "        # Check if the download directory exists, otherwise create it.\n",
        "        if not os.path.exists(download_dir):\n",
        "            os.makedirs(download_dir)\n",
        "\n",
        "        print(\"Downloading\", filename, \"...\")\n",
        "\n",
        "        # Download the file from the internet.\n",
        "        url = base_url + filename\n",
        "        file_path, _ = urllib.request.urlretrieve(url=url,\n",
        "                                                  filename=save_path)\n",
        "\n",
        "        print(\" Done!\")\n",
        "\n",
        "\n",
        "def maybe_download_and_extract(url=data_url, download_dir=hparams.data_dir):\n",
        "    \"\"\"\n",
        "    Download and extract the data if it doesn't already exist.\n",
        "    Assumes the url is a tar-ball file.\n",
        "    :param url:\n",
        "        Internet URL for the tar-file to download.\n",
        "        Example: \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
        "    :param download_dir:\n",
        "        Directory where the downloaded file is saved.\n",
        "        Example: \"data/CIFAR-10/\"\n",
        "    :return:\n",
        "        Nothing.\n",
        "    \"\"\"\n",
        "\n",
        "    # Filename for saving the file downloaded from the internet.\n",
        "    # Use the filename from the URL and add it to the download_dir.\n",
        "    filename = url.split('/')[-1]\n",
        "    file_path = os.path.join(download_dir, filename)\n",
        "\n",
        "    # Check if the file already exists.\n",
        "    # If it exists then we assume it has also been extracted,\n",
        "    # otherwise we need to download and extract it now.\n",
        "    if not os.path.exists(file_path):\n",
        "        # Check if the download directory exists, otherwise create it.\n",
        "        if not os.path.exists(download_dir):\n",
        "            os.makedirs(download_dir)\n",
        "\n",
        "        # Download the file from the internet.\n",
        "        file_path, _ = urllib.request.urlretrieve(url=url,\n",
        "                                                  filename=file_path)\n",
        "\n",
        "        print()\n",
        "        print(\"Download finished. Extracting files.\")\n",
        "\n",
        "        if file_path.endswith(\".zip\"):\n",
        "            # Unpack the zip-file.\n",
        "            zipfile.ZipFile(file=file_path, mode=\"r\").extractall(download_dir)\n",
        "        elif file_path.endswith((\".tar.gz\", \".tgz\")):\n",
        "            # Unpack the tar-ball.\n",
        "            tarfile.open(name=file_path, mode=\"r:gz\").extractall(download_dir)\n",
        "\n",
        "        print(\"Done.\")\n",
        "    else:\n",
        "        print(\"Data has apparently already been downloaded and unpacked.\")\n",
        "\n",
        "\n",
        "def _get_file_path(filename=\"\"):\n",
        "    \"\"\"\n",
        "    Return the full path of a data-file for the data-set.\n",
        "\n",
        "    If filename==\"\" then return the directory of the files.\n",
        "    \"\"\"\n",
        "\n",
        "    return os.path.join(hparams.data_dir, \"cifar-10-batches-py/\", filename)\n",
        "\n",
        "\n",
        "def _unpickle(filename):\n",
        "    \"\"\"\n",
        "    Unpickle the given file and return the data.\n",
        "\n",
        "    Note that the appropriate dir-name is prepended the filename.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create full path for the file.\n",
        "    file_path = _get_file_path(filename)\n",
        "\n",
        "    print(\"Loading data: \" + file_path)\n",
        "\n",
        "    with open(file_path, mode='rb') as file:\n",
        "        # In Python 3.X it is important to set the encoding,\n",
        "        # otherwise an exception is raised here.\n",
        "        data = pickle.load(file,encoding='bytes')\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def _convert_images(raw):\n",
        "    \"\"\"\n",
        "    Convert images from the CIFAR-10 format and\n",
        "    return a 4-dim array with shape: [image_number, height, width, channel]\n",
        "    where the pixels are floats between 0.0 and 1.0.\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert the raw images from the data-files to floating-points.\n",
        "    raw_float = np.array(raw, dtype=float) / 255.0\n",
        "\n",
        "    # Reshape the array to 4-dimensions.\n",
        "    images = raw_float.reshape([-1, num_channels, img_size, img_size])\n",
        "\n",
        "    # Reorder the indices of the array.\n",
        "    images = images.transpose([0, 2, 3, 1])\n",
        "\n",
        "    return images\n",
        "\n",
        "\n",
        "def _load_data(filename):\n",
        "    \"\"\"\n",
        "    Load a pickled data-file from the CIFAR-10 data-set\n",
        "    and return the converted images (see above) and the class-number\n",
        "    for each image.\n",
        "    \"\"\"\n",
        "\n",
        "    # Load the pickled data-file.\n",
        "    data = _unpickle(filename)\n",
        "\n",
        "    # Get the raw images.\n",
        "    raw_images = data[b'data']\n",
        "\n",
        "    # Get the class-numbers for each image. Convert to numpy-array.\n",
        "    cls = np.array(data[b'labels'])\n",
        "\n",
        "    # Convert the images.\n",
        "    images = _convert_images(raw_images)\n",
        "\n",
        "    return images, cls\n",
        "\n",
        "\n",
        "def load_class_names():\n",
        "    # Load the class-names from the pickled file.\n",
        "    raw = _unpickle(filename=\"batches.meta\")[b'label_names']\n",
        "\n",
        "    # Convert from binary strings.\n",
        "    names = [x.decode('utf-8') for x in raw]\n",
        "\n",
        "    return names\n",
        "\n",
        "\n",
        "def load_training_data():\n",
        "    \"\"\"\n",
        "    Load all the training-data for the CIFAR-10 data-set.\n",
        "\n",
        "    The data-set is split into 5 data-files which are merged here.\n",
        "\n",
        "    Returns the images, class-numbers and one-hot encoded class-labels.\n",
        "    \"\"\"\n",
        "\n",
        "    # Pre-allocate the arrays for the images and class-numbers for efficiency.\n",
        "    images = np.zeros(shape=[_num_images_train, img_size, img_size, num_channels], dtype=float)\n",
        "    cls = np.zeros(shape=[_num_images_train], dtype=int)\n",
        "\n",
        "    # Begin-index for the current batch.\n",
        "    begin = 0\n",
        "\n",
        "    # For each data-file.\n",
        "    for i in range(_num_files_train):\n",
        "        # Load the images and class-numbers from the data-file.\n",
        "        images_batch, cls_batch = _load_data(filename=\"data_batch_\" + str(i + 1))\n",
        "\n",
        "        # Number of images in this batch.\n",
        "        num_images = len(images_batch)\n",
        "\n",
        "        # End-index for the current batch.\n",
        "        end = begin + num_images\n",
        "\n",
        "        # Store the images into the array.\n",
        "        images[begin:end, :] = images_batch\n",
        "\n",
        "        # Store the class-numbers into the array.\n",
        "        cls[begin:end] = cls_batch\n",
        "\n",
        "        # The begin-index for the next batch is the current end-index.\n",
        "        begin = end\n",
        "\n",
        "    return images, cls\n",
        "\n",
        "\n",
        "def load_validation_data():\n",
        "\n",
        "    images, cls = _load_data(filename=\"test_batch\")\n",
        "\n",
        "    images = images[5000:, :, :, :]\n",
        "    cls = cls[5000:]\n",
        "\n",
        "    return images, cls\n",
        "\n",
        "def load_testing_data():\n",
        "\n",
        "\n",
        "    images, cls = _load_data(filename=\"test_batch\")\n",
        "\n",
        "    images = images[:5000, :, :, :]\n",
        "    cls = cls[:5000]\n",
        "\n",
        "    return images, cls\n",
        "  \n",
        "maybe_download_and_extract()\n",
        "\n",
        "# training data\n",
        "x_train, y_train = load_training_data()\n",
        "print(\"CIFAR-10 Training Dataset Size: \", x_train.shape)\n",
        "print(\"CIFAR-10 Training Labels Size : \", y_train.shape)\n",
        "\n"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data has apparently already been downloaded and unpacked.\n",
            "Loading data: /tmp/cifar-data/cifar-10-batches-py/data_batch_1\n",
            "Loading data: /tmp/cifar-data/cifar-10-batches-py/data_batch_2\n",
            "Loading data: /tmp/cifar-data/cifar-10-batches-py/data_batch_3\n",
            "Loading data: /tmp/cifar-data/cifar-10-batches-py/data_batch_4\n",
            "Loading data: /tmp/cifar-data/cifar-10-batches-py/data_batch_5\n",
            "CIFAR-10 Training Dataset Size:  (50000, 32, 32, 3)\n",
            "CIFAR-10 Training Labels Size :  (50000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFIzRwBy-QEz",
        "colab_type": "text"
      },
      "source": [
        "Defining Some Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1r92Sts41iUG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def conv_layer(input, filters, k_size = 3, strides = (1, 1), normalization = True, activation = tf.nn.leaky_relu, training = True, name = None):\n",
        "  conv = tf.layers.conv2d(input, filters, k_size, strides, padding = 'same', name = name)\n",
        "  if normalization:\n",
        "    conv = tf.layers.batch_normalization(conv, training = training, trainable = False)\n",
        "  if activation is not None:\n",
        "    out = activation(conv)\n",
        "  else:\n",
        "    out = conv\n",
        "  return out\n",
        "\n",
        "def deconv_layer(input, filters, k_size = 3, strides = (2, 2), normalization = True, activation = tf.nn.relu, training = True, name = None):\n",
        "  deconv = tf.layers.conv2d_transpose(input, filters, k_size, strides, padding = 'same', name = name)\n",
        "  if normalization:\n",
        "    deconv = tf.layers.batch_normalization(deconv, training = training)\n",
        "  if activation is not None:\n",
        "    out = activation(deconv)\n",
        "  else:\n",
        "    out = deconv\n",
        "  return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXl6sNfx3jL_",
        "colab_type": "text"
      },
      "source": [
        "Defining Generator and Discriminator models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJ_JqjEN1BBM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reset_default_graph()\n",
        "def generator(latent_z, reuse = False):\n",
        "  with tf.variable_scope('Generator', reuse = reuse) as scope:\n",
        "    linear = tf.layers.dense(latent_z, 4 * 4 * 512, activation = tf.nn.relu, name = 'dense')\n",
        "    linear_reshape = tf.reshape(linear, (-1, 4, 4, 512))\n",
        "    conv_1 = deconv_layer(linear_reshape, filters = 128, k_size = 3, name = 'conv_1')\n",
        "    conv_2 = deconv_layer(conv_1, filters = 64, k_size = 3, name = 'conv_2')\n",
        "    conv_3 = deconv_layer(conv_2, filters = num_channels, k_size = 3, name = 'conv_3')\n",
        "  return conv_3\n",
        "\n",
        "def discriminator(image, reuse = False):\n",
        "  with tf.variable_scope('Discriminator', reuse = reuse) as scope:\n",
        "    conv_1 = conv_layer(image, filters = 64, k_size = 3, strides = (2, 2), name = 'conv_1')\n",
        "    conv_2 = conv_layer(conv_1, filters = 128, k_size = 3, strides = (2, 2), name = 'conv_2')\n",
        "    conv_3 = conv_layer(conv_2, filters = 256, k_size = 3, strides = (2, 2), name = 'conv_3')\n",
        "    flat = tf.reshape(conv_3, shape = [-1, 4 * 4 * 256])\n",
        "    logits = tf.layers.dense(flat, 1, name = 'dense')\n",
        "  return logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SER9CfKq3yKy",
        "colab_type": "text"
      },
      "source": [
        "Defining Generator and Discriminator Loss and Training Step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MgQ-agC38-s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = hparams.batch_size\n",
        "real_image = tf.placeholder(tf.float32, shape = (None, img_size, img_size, num_channels))\n",
        "latent_z = tf.random.normal(shape = (batch_size, 128))\n",
        "fake_image = generator(latent_z)\n",
        "\n",
        "real_dis = discriminator(real_image)\n",
        "fake_dis = discriminator(fake_image, reuse = True)\n",
        "\n",
        "# print(real_dis.shape)\n",
        "# print(fake_dis.shape)\n",
        "\n",
        "real_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = real_dis, labels = tf.ones_like(real_dis)))\n",
        "fake_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = fake_dis, labels = tf.zeros_like(fake_dis)))\n",
        "dis_loss = 0.5 * (real_loss + fake_loss)\n",
        "\n",
        "# print(real_loss.shape)\n",
        "# print(fake_loss.shape)\n",
        "\n",
        "gen_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = fake_dis, labels = tf.ones_like(fake_dis)))\n",
        "\n",
        "gen_variables = [v for v in tf.trainable_variables() if 'Generator' in v.name]\n",
        "dis_variables = [v for v in tf.trainable_variables() if 'Discriminator' in v.name]\n",
        "# for op in gen_variables:\n",
        "#   print(op.name)\n",
        "# for op in dis_variables:\n",
        "#   print(op.name)\n",
        "\n",
        "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "update_ops_gen = [op for op in update_ops if 'Generator' in op.name]\n",
        "update_ops_dis = [op for op in update_ops if 'Discriminator' in op.name]\n",
        "# for op in update_ops_gen:\n",
        "#   print(op.name)\n",
        "gen_optimizer = tf.train.AdamOptimizer(learning_rate=hparams.gen_learning_rate)\n",
        "gen_train_step = tf.group([gen_optimizer.minimize(gen_loss, var_list = gen_variables), update_ops_gen])\n",
        "dis_optimizer = tf.train.AdamOptimizer(learning_rate=hparams.dis_learning_rate)\n",
        "dis_train_step = tf.group([dis_optimizer.minimize(dis_loss, var_list = dis_variables), update_ops_dis])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0DUfACmMG9j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_generated(generated_images, step):\n",
        "  batch_size, channels, height, width = generated_images.shape\n",
        "  length = batch_size // 4\n",
        "  generated_images = (generated_images) * 255.0\n",
        "  generated_images[generated_images > 255.0] = 255.0\n",
        "  generated_images[generated_images < 0.0] = 0.0\n",
        "  for i in range(batch_size):\n",
        "    plt.subplot(4, length, i + 1)\n",
        "    plt.axis('off')\n",
        "    # image = np.transpose(generated_images[i].astype('uint8'), (1, 2, 0))\n",
        "    image = generated_images[i].astype('uint8')\n",
        "    # image = image[:, :, ::-1]\n",
        "    plt.imshow(image)\n",
        "  plt.savefig('generated_images_{}.png'.format(step))\n",
        "  plt.close()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9k7xTAP6kXv",
        "colab_type": "text"
      },
      "source": [
        "Training Iterations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZXmYaVexzJ43",
        "outputId": "f2c0c00c-b7c3-4024-b3df-2d087a51e49c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 660
        }
      },
      "source": [
        "epochs = hparams.n_epochs\n",
        "steps = _num_images_train // batch_size\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "for epoch in range(epochs):\n",
        "  for step in range(steps):\n",
        "    real_image_, dis_loss_, _ = sess.run([real_image, dis_loss, dis_train_step], feed_dict = {real_image: x_train[step * batch_size : (step + 1) * batch_size, :, :, :]})\n",
        "    fake_image_, gen_loss_, _ = sess.run([fake_image, gen_loss, gen_train_step])\n",
        "  print(\"Epoch: {}  Discriminator loss: {}  Generator loss {}\".format(epoch, dis_loss_, gen_loss_))\n",
        "  save_generated(fake_image_, epoch)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0  Discriminator loss: 0.002415051683783531  Generator loss 6.421932220458984\n",
            "Epoch: 1  Discriminator loss: 0.005110227037221193  Generator loss 6.6422505378723145\n",
            "Epoch: 2  Discriminator loss: 0.05788949877023697  Generator loss 7.782761573791504\n",
            "Epoch: 3  Discriminator loss: 0.28959348797798157  Generator loss 4.0311479568481445\n",
            "Epoch: 4  Discriminator loss: 0.1565711498260498  Generator loss 4.419004440307617\n",
            "Epoch: 5  Discriminator loss: 0.13973888754844666  Generator loss 3.687959671020508\n",
            "Epoch: 6  Discriminator loss: 0.032819610089063644  Generator loss 2.6049423217773438\n",
            "Epoch: 7  Discriminator loss: 0.14029867947101593  Generator loss 3.1812143325805664\n",
            "Epoch: 8  Discriminator loss: 0.02636696770787239  Generator loss 6.17720365524292\n",
            "Epoch: 9  Discriminator loss: 0.07773062586784363  Generator loss 3.842236280441284\n",
            "Epoch: 10  Discriminator loss: 0.055329300463199615  Generator loss 2.9127914905548096\n",
            "Epoch: 11  Discriminator loss: 0.10469095408916473  Generator loss 3.331754207611084\n",
            "Epoch: 12  Discriminator loss: 0.4429153800010681  Generator loss 1.2800874710083008\n",
            "Epoch: 13  Discriminator loss: 0.12808758020401  Generator loss 2.229185104370117\n",
            "Epoch: 14  Discriminator loss: 0.08944617211818695  Generator loss 3.3972506523132324\n",
            "Epoch: 15  Discriminator loss: 0.2501111328601837  Generator loss 2.002901554107666\n",
            "Epoch: 16  Discriminator loss: 0.4866030216217041  Generator loss 2.2501323223114014\n",
            "Epoch: 17  Discriminator loss: 0.4158249795436859  Generator loss 1.3861429691314697\n",
            "Epoch: 18  Discriminator loss: 0.31295761466026306  Generator loss 1.986070990562439\n",
            "Epoch: 19  Discriminator loss: 0.2769506573677063  Generator loss 3.2980427742004395\n",
            "Epoch: 20  Discriminator loss: 0.14751602709293365  Generator loss 1.8956782817840576\n",
            "Epoch: 21  Discriminator loss: 0.14667366445064545  Generator loss 2.1025030612945557\n",
            "Epoch: 22  Discriminator loss: 0.326094388961792  Generator loss 2.028627872467041\n",
            "Epoch: 23  Discriminator loss: 0.5944232940673828  Generator loss 1.1547666788101196\n",
            "Epoch: 24  Discriminator loss: 0.11831758171319962  Generator loss 1.2071723937988281\n",
            "Epoch: 25  Discriminator loss: 0.06745409965515137  Generator loss 2.2379770278930664\n",
            "Epoch: 26  Discriminator loss: 0.5338824987411499  Generator loss 1.5633800029754639\n",
            "Epoch: 27  Discriminator loss: 0.15494655072689056  Generator loss 1.5809664726257324\n",
            "Epoch: 28  Discriminator loss: 0.323650062084198  Generator loss 1.051051139831543\n",
            "Epoch: 29  Discriminator loss: 0.1349715292453766  Generator loss 3.1112661361694336\n",
            "Epoch: 30  Discriminator loss: 0.453931599855423  Generator loss 1.4652152061462402\n",
            "Epoch: 31  Discriminator loss: 0.6664190888404846  Generator loss 1.8968465328216553\n",
            "Epoch: 32  Discriminator loss: 0.12026186287403107  Generator loss 1.5370546579360962\n",
            "Epoch: 33  Discriminator loss: 0.1326097846031189  Generator loss 2.8626394271850586\n",
            "Epoch: 34  Discriminator loss: 0.14272713661193848  Generator loss 3.3367631435394287\n",
            "Epoch: 35  Discriminator loss: 0.35943764448165894  Generator loss 1.4286987781524658\n",
            "Epoch: 36  Discriminator loss: 0.15443918108940125  Generator loss 1.7995777130126953\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gj9x1qefsxM7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Image(filename='generated_images_{}.png'.format(epochs - 1)) "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}